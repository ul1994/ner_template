{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- -X- O\r\n",
      "\r\n",
      "EU NNP I-NP I-ORG\r\n",
      "rejects VBZ I-VP O\r\n",
      "German JJ I-NP I-MISC\r\n",
      "call NN I-NP O\r\n",
      "to TO I-VP O\r\n",
      "boycott VB I-VP O\r\n",
      "British JJ I-NP I-MISC\r\n",
      "lamb NN I-NP O\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 eng.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14986"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('eng.train') as fl:\n",
    "    sents = fl.read().split('\\n\\n')[1:-1]\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14986\n",
      "8\n",
      "['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "# [[(word, label), (...), ...] , [...], [...]]\n",
    "ner_tags = {}\n",
    "for meta in sents:\n",
    "    pairs = []\n",
    "    for line in meta.split('\\n'):\n",
    "        entries = line.split()\n",
    "        # word, NER tag\n",
    "        pairs.append([entries[0], entries[-1]])\n",
    "        ner_tags[entries[-1]] = True\n",
    "    sentences.append(pairs)\n",
    "    \n",
    "ner_tags = list(ner_tags.keys())\n",
    "\n",
    "print(len(sentences))\n",
    "print(len(ner_tags))\n",
    "print(sorted(ner_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on a IOB2 tag ordering which we will use to build and read 1-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = ['B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-ORG', 'I-ORG', 'I-PER', 'O']\n",
    "for ent in lookup:\n",
    "    assert ent in ner_tags # sanity check\n",
    "ldict = { tag: ind for ind, tag in enumerate(lookup) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lookup.json', 'w') as fl:\n",
    "    json.dump(ldict, fl, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"I-LOC\": 1,\r\n",
      "    \"O\": 7,\r\n",
      "    \"I-ORG\": 3,\r\n",
      "    \"B-MISC\": 5,\r\n",
      "    \"B-LOC\": 0,\r\n",
      "    \"B-ORG\": 2,\r\n",
      "    \"I-MISC\": 6,\r\n",
      "    \"B-PER\": 4\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "# From now on, we will use index values to refer to NER tags\n",
    "!cat lookup.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Google embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "import gensim\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 145.99s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "print('Loaded: %.2fs' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words (found in embedding): 23625\n",
      "Missing in embedding: 5900\n"
     ]
    }
   ],
   "source": [
    "missing = {}\n",
    "uniques = {}\n",
    "for sent in sentences:\n",
    "    for word, tag in sent:\n",
    "        if word not in model:\n",
    "            if word not in missing:\n",
    "                missing[word] = True\n",
    "#                 print('Not in embedding:', word)\n",
    "                uniques[word] = model['unk']\n",
    "        else:\n",
    "            uniques[word] = model[word]\n",
    "\n",
    "uniques['unk'] = model['unk'] # also save unk for later use\n",
    "\n",
    "print('Unique words (found in embedding):', len(uniques))\n",
    "print('Missing in embedding:', len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zinc', 'zinfandel', 'zlotys', 'zone', 'zvezda']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_words = sorted(list(uniques.keys()))\n",
    "subset_words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_list.txt', 'w') as fl:\n",
    "    fl.write('\\n'.join(subset_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "emat = np.zeros((len(subset_words), 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for si, word in enumerate(subset_words):\n",
    "    emat[si, :] = uniques[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_embeds.npy', emat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
